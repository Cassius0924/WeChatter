from typing import Dict, List
from urllib.parse import quote

import requests
from bs4 import BeautifulSoup

from wechatter.commands.handlers import command
from wechatter.exceptions import Bs4ParsingError
from wechatter.models.message import SendMessage, SendMessageType, SendTo
from wechatter.sender import Sender
from wechatter.utils import get_request


@command(
    command="food-calories",
    keys=["é£Ÿç‰©çƒ­é‡", "food-calories", "çƒ­é‡", "calories", "å¡è·¯é‡Œ"],
    desc="è·å–é£Ÿç‰©çƒ­é‡ã€‚",
)
def food_calories_command_handler(to: SendTo, message: str = "") -> None:
    try:
        response = get_request(
            url=f"https://www.boohee.com/food/search?keyword={message}"
        )
        food_href_list = parse_food_href_list_response(response)
        food_detail_list = get_food_detail_list(food_href_list)
        result = generate_food_message(food_detail_list)
        Sender.send_msg(to, SendMessage(SendMessageType.TEXT, result))
    except Exception as e:
        error_message = f"è·å–é£Ÿç‰©çƒ­é‡å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{e}"
        print(error_message)
        Sender.send_msg(to, SendMessage(SendMessageType.TEXT, error_message))


def get_food_detail_list(food_href_list: List) -> List:
    """
    è·å–é£Ÿç‰©è¯¦æƒ…åˆ—è¡¨
    :param food_href_list: é£Ÿç‰©è¯¦æƒ…é“¾æ¥åˆ—è¡¨
    :return: é£Ÿç‰©è¯¦æƒ…åˆ—è¡¨
    """
    food_detail_list = []
    for i, food in enumerate(food_href_list[:5]):
        food_name = food.get("name")
        food_all_name = food.get("all_name")
        food_href = food.get("href")
        keyword = get_url_encoding(food_name)  # è·å–URLç¼–ç 
        headers = {
            "referer": f"https://www.boohee.com/food/search?keyword={keyword}",
        }
        food_response = get_request(
            url=f"https://www.boohee.com{food_href}", headers=headers
        )
        if not food_response:
            raise Exception(f"è·å–é£Ÿç‰©è¯¦æƒ…å¤±è´¥ï¼Œé£Ÿç‰©åç§°ï¼š{food_name}")
        food_detail = parse_food_detail_response(food_response, food_all_name)
        food_detail_list.append(food_detail)
    if not food_detail_list:
        raise Exception("è·å–é£Ÿç‰©è¯¦æƒ…å¤±è´¥,ä¸ºç©ºåˆ—è¡¨")
    return food_detail_list


def generate_food_message(food_detail_list: List) -> str:
    """
    ç”Ÿæˆé£Ÿç‰©ä¿¡æ¯
    :param food_detail_list: é£Ÿç‰©è¯¦æƒ…åˆ—è¡¨
    :return: é£Ÿç‰©ä¿¡æ¯
    """
    food_str = "âœ¨=====é£Ÿç‰©åˆ—è¡¨=====âœ¨\n"

    for i, food_detail in enumerate(food_detail_list):
        energy = food_detail["çƒ­é‡(å¤§å¡)"]
        carbohydrate = food_detail["ç¢³æ°´åŒ–åˆç‰©(å…‹)"]
        fat = food_detail["è„‚è‚ª(å…‹)"]
        protein = food_detail["è›‹ç™½è´¨(å…‹)"]
        dietary_fiber = food_detail["çº¤ç»´ç´ (å…‹)"]
        food_all_name = food_detail["food_all_name"]
        food_str += (
            f"{i + 1}. {food_all_name}\n"
            f"    ğŸ²çƒ­é‡(å¤§å¡):    {energy}\n"
            f"    ğŸç¢³æ°´(å…‹):        {carbohydrate}\n"
            f"    ğŸ¥“è„‚è‚ª(å…‹):        {fat}\n"
            f"    ğŸ—è›‹ç™½è´¨(å…‹):    {protein}\n"
            f"    ğŸ¥¦çº¤ç»´ç´ (å…‹):    {dietary_fiber}\n"
        )
    food_str += "ğŸ”µ====å«é‡(100å…‹)====ğŸ”µ"

    return food_str


# è¿™é‡Œä¹Ÿæ˜¯ parseéƒ¨åˆ†
def parse_food_detail_response(response: requests.Response, food_all_name: str) -> Dict:
    """
    è§£æé£Ÿç‰©è¯¦æƒ…
    :param response: è¯·æ±‚å“åº”
    :param food_all_name: é£Ÿç‰©å…¨å
    :return: é£Ÿç‰©è¯¦æƒ…
    """
    print(response.text)
    soup = BeautifulSoup(response.text, "html.parser")
    food_detail = {}
    articles = soup.find_all("dd")
    for article in articles:
        name = article.select_one("span.dt").text.strip()
        value = article.select_one("span.dd").text.strip()
        if name and value:
            food_detail[name] = value
    if not food_detail:
        raise Bs4ParsingError("è§£æé£Ÿç‰©åˆ—è¡¨å¤±è´¥")

    food_detail["food_all_name"] = food_all_name
    return food_detail


def parse_food_href_list_response(response: requests.Response) -> List:
    """
    è§£æé£Ÿç‰©è¯¦æƒ…é“¾æ¥åˆ—è¡¨
    :param response: è¯·æ±‚å“åº”
    :return: é£Ÿç‰©è¯¦æƒ…é“¾æ¥åˆ—è¡¨
    """
    soup = BeautifulSoup(response.text, "html.parser")
    href_list = []
    articles = soup.select("div.text-box")
    for article in articles:
        href_list_item = {}

        name = article.select_one("a").text.strip()
        if name:
            href_list_item["name"] = name.split("ï¼Œ")[0]
            href_list_item["all_name"] = name
        href = article.a["href"]
        if href:
            href_list_item["href"] = href

        if href_list_item:
            href_list.append(href_list_item)

    if not href_list:
        raise Bs4ParsingError("è§£æé£Ÿç‰©è¯¦æƒ…é“¾æ¥å¤±è´¥")

    return href_list


def get_url_encoding(message: str) -> str:
    """
    è·å–URLç¼–ç 
    :param message: æ¶ˆæ¯
    :return: URLç¼–ç 
    """
    url_encoding = quote(message)
    return url_encoding
